---
title: "Dimensionality Reduction and Clustering"
---

Dimensionality reduction often takes complex multidimensional data (like an M x N matrix of insertion counts per sample) and reduces this object to a 2 dimensional plot for visualization purposes. However, in scATAC-seq, the multidimensional insertion counts matrix is extremely _sparse_ because each cell (generally) has two alleles and transposition is a relatively rare event. So even if a region is accessible, it may not be transposed. The resultant insertion counts matrix is trinary (0s, 1s, and 2s) and we convert this to a binary matrix (0 = not accessible, 1 = accessible). This binary matrix ends up being mostly 0s because transposition is rare. However, it is important to note that a 0 in scATAC-seq could mean "non-accessible" or "not sampled" and these two inferences are very different from a biological standpoint. Because of this, the 1s have information and the 0s do not. This low information content makes our scATAC-seq data _sparse_.

If you were to perform a standard dimensionality reduction, like Principal Component Analysis, on this sparse insertion counts matrix and plot the top two principal components, you would not obtain the desired result because the sparsity causes high inter-cell similarity at all of the 0 positions. To get around this issue, we use a layered dimensionality reduction approach. First, we use Latent Semantic Indexing (LSI), an approach from natural language processing that was originally designed to assess document similarity based on word counts. This solution was created for natural language processing because the data is sparse and noisy (many different words and many low frequency words). In the case of scATAC-seq, different samples are the _documents_ and different regions/peaks are the _words_. First, we calculate the term frequency-inverse document frequency (TF-IDF) which reflects how important a _word_ (aka region/peak) is to a _document_ (aka sample).  Then, through a technique called singular value decomposition (SVD), the most _valuable_ information across samples is identified and represented in a lower dimensional space. LSI allows you to reduce the dimensionality of the sparse insertion counts matrix from many thousands to tens or hundreds. Then, a more conventional dimensionality reduction technique, such as Uniform Manifold Approximation and Projection (UMAP) or t-distributed stochastic neighbor embedding (t-SNE) can be used to visualize the data. In ArchR, these visualization methods are referred to as _embeddings_.

ArchR implements an iterative LSI approach and we recommend using a tiled genome-wide matrix as input. This allows for the identification of clusters prior to calling peaks which provides many advantages both in terms of speed and bias. The iterative approach is important because __QQQ__. All of this happens under the hood using the `addIterativeLSI()` function. You can tweak all of the parameters including the matrix used (`useMatrix`), the number of `iterations`, the top N dimensions to use (`dimsToUse`), and many others.

![](peakCalling_Comparison.png){width=350px}

The last piece of the dimensionality reduction puzzle in scATAC-seq is the removal of batch effects which is a complex problem to address. The `addIterativeLSI()` function allows for the use of [Harmony](https://github.com/immunogenomics/harmony) to remove batch effects via the `runHarmony` and `harmonyParams` parameters. This approach performs well in our hands but is not set to run by default. Users should be aware of the caveats of batch correction for their particular application.

[INSERT PRE- AND POST-HARMONY UMAPS]